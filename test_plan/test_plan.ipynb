{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"CSCI3100: Software Engineering\"\n",
    "subtitle: \"Test Plan of AIO - Course Planner\"\n",
    "author: \"\"\n",
    "institute: \"Department of Computer Science And Engineering, The Chinese University of Hong Kong\"\n",
    "date: \"23 Apr 2025\"\n",
    "date-format: long\n",
    "format:\n",
    "  pdf:\n",
    "    toc: true\n",
    "    number-sections: true\n",
    "    colorlinks: true\n",
    "    include-in-header: \n",
    "      text: |\n",
    "        \\usepackage{fancyhdr}\n",
    "        \\pagestyle{fancy}\n",
    "execute:\n",
    "  echo: true\n",
    "  warning: true\n",
    "  output: true\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Revision History\n",
    "\n",
    "|Version| Revised By| Revision Date| Comments|\n",
    "|-------|-----------|--------------|---------|\n",
    "|1.0    | Anson     | 23 Apr 2025  | Initial draft| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test Plan for **AIO - Course Planner**\n",
    "\n",
    "This is a test plan for **AIO - Course Planner**. The most important sections include:\n",
    "\n",
    "1. Scope and Objectives --- This part states what will be (and what not will be) included in the testing.\n",
    "2. Test Cases and Scenarios --- This part explains each test case and scenario, by providing a detailed breakdown of the scope, the testing procedures and pass/fail criteria.\n",
    "3. Team Roles and Responsibilities --- This part shows the assignment of specific duties and accountabilities to each team member.\n",
    "4. Testing Approach & Timeline and Schedule --- This part outlines the approach and schedule of the tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scope and Objectives\n",
    "This part defines the scope of test cases. By having a clear definition, it avoids multiple test cases overlapping on certain areas.\n",
    "\n",
    "### Scope:\n",
    "The test plan focuses on ensuring the functionality, performance, and user experience of the **AIO - Course Planner**. The following areas will be tested:\n",
    "\n",
    "- Core mechanics (e.g., Timetable planning, Study Plan planner, Viewing degree roadmap, Course commenting).\n",
    "- User interface (UI) and user experience (UX).\n",
    "- Account system (Login, Logout, Registration).\n",
    "- Email token verification module.\n",
    "- Save/load functionality (for Timetable and Study Plan).\n",
    "- Third-party integration to Google Calendar.\n",
    "\n",
    "### Out of Scope:\n",
    "- Some hard-to-test aspects of the software (please see Risk assessment and mitigation).\n",
    "\n",
    "### Objectives:\n",
    "- Verify that the system meets functional and non-functional requirements, and provides a bug-free experience.\n",
    "- Ensure the system is intuitive, and free of critical bugs.\n",
    "- Confirm that the system performs well under various conditions.\n",
    "- Validate that the system is ready for release and meets quality standards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Cases and Scenarios\n",
    "The section outlines comprehensive testing strategies for the **AIO - Course Planner** web application. It has test cases for both functional and non-functional test scenarios designed to ensure the application meets quality standards and user requirements. Each test case includes procedures, expected results, and criteria to guide manual testing efforts into the testing process.\n",
    "\n",
    "The test cases below use manual testing and a testing framework called Cypress which is known for its E2E (end-to-end) testing ability which simulates the real user experience.\n",
    "\n",
    "### Test Cases for Functional Requirements:\n",
    "1. User Authentication:\n",
    "   - Steps: Register a new account, verify email, and login with credentials.\n",
    "   - Expected Result: User is registered successfully and can log in without email verification (it is required for course commenting only).\n",
    "   - Pass/Fail Criteria: Account is not created in database or users cannot log in.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC001\n",
    "       - Repository Link: [GitHub - auth.cy.js](https://github.com/le-lel190/csci3100-project/blob/dev_testing/cypress/e2e/auth.cy.js)\n",
    "\n",
    "2. Course Search and Selection:\n",
    "   - Steps: Search for courses by keyword, filter results, and view course details.\n",
    "   - Expected Result: Relevant courses are displayed and details are accessible.\n",
    "   - Pass/Fail Criteria: Course search returns accurate results and course information is complete.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC002\n",
    "       - Repository Link: [GitHub - course-search.cy.js](https://github.com/le-lel190/csci3100-project/blob/dev_testing/cypress/e2e/course-search.cy.js)\n",
    "       - Status: Automated with Cypress\n",
    "       - Coverage: Basic keyword search and semester filtering implemented\n",
    "\n",
    "3. Study Plan Creation:\n",
    "   - Steps: Add courses to study plan, modify sequence, and save changes.\n",
    "   - Expected Result: Study plan is updated and persisted correctly.\n",
    "   - Pass/Fail Criteria: Changes to study plan are saved and retrieved accurately on reload.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC003\n",
    "       - Repository Link: [GitHub - study-plan.cy.js](https://github.com/le-lel190/csci3100-project/blob/dev_testing/cypress/e2e/study-plan.cy.js)\n",
    "       - Status: Partially automated with Cypress\n",
    "       - Coverage: Adding courses and saving study plan implemented. Course reordering (drag-and-drop) currently skipped in automation.\n",
    "\n",
    "4. Timetable Management:\n",
    "   - Steps: Add courses to timetable, and detect conflicts.\n",
    "   - Expected Result: Courses appear in timetable at correct times with conflict warnings.\n",
    "   - Pass/Fail Criteria: Timetable visually displays all courses without UI errors.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC004\n",
    "       - Status: Manual testing only\n",
    "\n",
    "5. Comment System:\n",
    "   - Steps: Add comments to courses, edit/delete own comments, and view others' comments.\n",
    "   - Expected Result: Comments are saved and displayed correctly for all users.\n",
    "   - Pass/Fail Criteria: Comment operations complete without errors and update in real-time.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC005\n",
    "       - Status: Manual testing only\n",
    "\n",
    "6. Degree Roadmap Visualization:\n",
    "   - Steps: View degree roadmap, interact with course nodes, and check prerequisites.\n",
    "   - Expected Result: Roadmap displays correct course sequence with prerequisites highlighted.\n",
    "   - Pass/Fail Criteria: All course relationships are accurately represented in the visualization.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC006\n",
    "       - Status: Manual testing only\n",
    "\n",
    "\n",
    "### Test Cases for Non-Functional Requirements\n",
    "\n",
    "This part focus on test cases that assess the non-functional requirements focus on the quality attributes of the performance, scalability, usability, reliability, and security.\n",
    "\n",
    "#### 1. Performance Testing\n",
    "\n",
    "1. Page Load Time\n",
    "   - Objective: Verify that pages load within acceptable time limits.\n",
    "   - Steps:\n",
    "      1. Navigate to each main page (index, timetable, study planner, etc.)\n",
    "      2. Measure the time taken to fully load content.\n",
    "   - Expected Result: Pages load within 2 seconds on standard connections.\n",
    "   - Pass/Fail Criteria: Pass if load time meets expectations; fail otherwise.\n",
    "   - Status: Basic load testing implemented in 0-app-ready.cy.js\n",
    "\n",
    "2. API Response Time\n",
    "   - Objective: Ensure API endpoints respond quickly to client requests.\n",
    "   - Steps:\n",
    "      1. Make requests to key API endpoints (course search, authentication, etc.)\n",
    "      2. Measure response times under different load conditions.\n",
    "   - Expected Result: API responses return within 500ms on average.\n",
    "   - Pass/Fail Criteria: Pass if response times remain within acceptable thresholds.\n",
    "   - Status: Basic API health check in 0-app-ready.cy.js\n",
    "\n",
    "3. Simultaneous User Testing\n",
    "   - Objective: Test application behavior with many concurrent users.\n",
    "   - Steps:\n",
    "      1. Simulate multiple users accessing the application simultaneously.\n",
    "      2. Monitor server resource usage and response times.\n",
    "   - Expected Result: Application remains responsive with up to 100 simultaneous users.\n",
    "   - Pass/Fail Criteria: Pass if performance degradation is minimal under load.\n",
    "   - Status: Manual testing only, hard to test\n",
    "\n",
    "\n",
    "#### 2. Usability Testing\n",
    "\n",
    "1. Navigation Flow\n",
    "   - Objective: Verify that users can navigate the application intuitively.\n",
    "   - Steps:\n",
    "      1. Ask new users to complete common tasks without instructions.\n",
    "      2. Observe navigation patterns and points of confusion.\n",
    "   - Expected Result: Users can complete basic tasks without assistance.\n",
    "   - Pass/Fail Criteria: Pass if users accomplish tasks efficiently; fail if significant confusion occurs.\n",
    "\n",
    "2. Mobile Responsiveness\n",
    "   - Objective: Ensure the application is usable on mobile devices.\n",
    "   - Steps:\n",
    "      1. Access application on various mobile devices and screen sizes.\n",
    "      2. Test all major features on mobile browsers.\n",
    "   - Expected Result: UI adapts appropriately to screen size with all functionality accessible.\n",
    "   - Pass/Fail Criteria: Pass if mobile experience is consistent with desktop.\n",
    "\n",
    "#### 3. Reliability Testing\n",
    "\n",
    "1. Data Persistence\n",
    "   - Objective: Verify that user data is saved correctly and persistently.\n",
    "   - Steps:\n",
    "      1. Create study plans and timetables, then log out and back in.\n",
    "      2. Check that all user-created data is preserved.\n",
    "   - Expected Result: All user data is persistent across sessions.\n",
    "   - Pass/Fail Criteria: Pass if no data loss occurs; fail otherwise.\n",
    "\n",
    "2. Error Recovery\n",
    "   - Objective: Test application's ability to handle and recover from errors.\n",
    "   - Steps:\n",
    "      1. Simulate network disruptions during operations.\n",
    "      2. Intentionally submit invalid data to test error handling.\n",
    "   - Expected Result: Application provides meaningful error messages and recovers gracefully.\n",
    "   - Pass/Fail Criteria: Pass if app maintains usability after errors.\n",
    "\n",
    "\n",
    "#### 4. Security Testing\n",
    "\n",
    "1. Authentication Security\n",
    "   - Objective: Verify secure login and session management.\n",
    "   - Steps:\n",
    "      1. Attempt various authentication bypass techniques.\n",
    "      2. Test password strength requirements and account lockout functionality.\n",
    "   - Expected Result: Unauthorized access attempts are prevented.\n",
    "   - Pass/Fail Criteria: Pass if authentication barriers cannot be circumvented.\n",
    "\n",
    "2. Data Protection\n",
    "   - Objective: Ensure user data is protected.\n",
    "   - Steps:\n",
    "      1. Inspect network traffic for sensitive data transmission.\n",
    "      2. Test that API endpoints properly validate authentication.\n",
    "   - Expected Result: Sensitive data is encrypted and access-controlled.\n",
    "   - Pass/Fail Criteria: Pass if no unauthorized data access is possible.\n",
    "\n",
    "#### 5. Compatibility Testing\n",
    "\n",
    "1. Browser Compatibility\n",
    "   - Objective: Ensure functionality across major browsers.\n",
    "   - Steps:\n",
    "      1. Test application on Chrome, Firefox, Safari, and Edge.\n",
    "      2. Verify feature parity and visual consistency.\n",
    "   - Expected Result: Application works identically across all supported browsers.\n",
    "   - Pass/Fail Criteria: Pass if no browser-specific issues exist.\n",
    "\n",
    "2. Device Compatibility\n",
    "   - Objective: Verify that the web-application works on different devices and screen sizes.\n",
    "   - Steps:\n",
    "      1. Test on desktop, tablet, and smartphones.\n",
    "      2. Check for layout or functionality issues.\n",
    "   - Expected Result: Application is fully functional.\n",
    "   - Pass/Fail Criteria: Pass if the UX is consistent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource Allocation\n",
    "This section outlines the personnel, tools, environments and other resources required to execute the test plan effectively.\n",
    "\n",
    "### 1. Team Roles and Responsibilities\n",
    "\n",
    "#### Key Team Members:\n",
    "1. Cheung Wai Lok Anson:\n",
    "    - The base of the system (backend, database).\n",
    "    - Mainly responsible for the timetable page.\n",
    "2. Lam Leung Yiu Jason:\n",
    "    - Mainly responsible for the degreeRoadmap page.\n",
    "3. Liu Yun Hei Jonathan:\n",
    "    - Mainly responsible for the study planner page.\n",
    "4. Cheung Ching Yu Eddy:\n",
    "    - Mainly responsible for the course comment page.\n",
    "5. All members:    \n",
    "    - Manages database configurations.\n",
    "    - Resolves server-side issues.\n",
    "    - Validate the user interface and user experience. \n",
    "    - Resolves front-end related issues.\n",
    "    - Document results. \n",
    "    - Identity and report bugs.\n",
    "\n",
    "#### Resource Allocation Table:\n",
    "\n",
    "|       Name            | Responsibilities      | Contribution (%) |\n",
    "|-----------------------|-----------------------|-------|\n",
    "| Cheung Wai Lok Anson  | Timetable page        | 25% |\n",
    "| Lam Leung Yiu Jason   | Degree roadmap page   | 25% |\n",
    "| Liu Yun Hei Jonathan  | Study Planner page    | 25% |\n",
    "| Cheung Ching Yu Eddy  | Course comment page   | 25% |\n",
    "\n",
    "\n",
    "### 2. Tools and Software\n",
    "\n",
    "#### Testing Tools:\n",
    "- Browser Developer Tools: Chrome DevTools for frontend testing.\n",
    "- MongoDB Compass: For database inspection and verification.\n",
    "- Manual Testing: Since no framework is used for webapp building.\n",
    "- Git: For version controlling test scripts/datasets/files and documents.\n",
    "\n",
    "### 3. Testing Environments\n",
    "\n",
    "#### Environment Types:\n",
    "- Virtual Machine (Docker): Both developing, testing and production environments are containerized by Docker to avoid endpoint discrepency.\n",
    "- Developing environment: Consists of test files and source codes, used by developers for developing, testing and debugging.\n",
    "- Production Environment: Used for final validation during User Acceptance Testing. Mimics the live environment to ensure the system is ready for release.\n",
    "\n",
    "#### Environment Setup:\n",
    "- Hardware Requirements:\n",
    "    - Devices/Browsers/DevTool mimicing to test behaviour of the web-application in desktop and laptop environments.\n",
    "\n",
    "- Software Requirements:\n",
    "    - Docker (Compose) Containers: For consistent testing environments.\n",
    "    - Node.js: The backend of the web-application.\n",
    "    - MongoDB Database: Stores user data and comments.\n",
    "\n",
    "### 4. Time Allocation\n",
    "\n",
    "#### Effort Estimation:\n",
    "\n",
    "- Effort Estimation:\n",
    "    1. Test Planning: 10% of the total effort\n",
    "    2. Test Case Development: 40% of the total effort\n",
    "    3. Functional Testing: 5% of the total effort\n",
    "    4. Non-functional Testing: 5% of the total effort\n",
    "    5. Bug Fixing and Verification: 40% of the total effort\n",
    "\n",
    "#### Time Allocation Table:\n",
    "\n",
    "| Testing Phase | Duration | Team Members Involved | Deliverables |\n",
    "|---------------|----------|----------------------|--------------|\n",
    "| Test Planning | 2 days | All team members | Test Plan Document |\n",
    "| Test Case Development | 8 days | All team members | Test Cases |\n",
    "| Manual testing: Test Case Execution | 1 day | Jason, Jonathan, Eddy | Test Results |\n",
    "| Automated testing: Test Case Execution | 1 day | Anson | Test Results |\n",
    "| Bug Fixing and Verification | 8 days | All team members | Working Product |\n",
    "\n",
    "\n",
    "### 5. Budget Allocation\n",
    "\n",
    "#### Key Budget Considerations:\n",
    "- Time Investment: Primary resource is team members' time.\n",
    "- Free Tools: Utilizing free versions instead of premium versions of software/services.\n",
    "- Personal Computing Resources: Using team members' own computers.\n",
    "- Cloud Resources: Minimal use of free tier cloud services if needed (e.g. using `EtherealMail` + `NodeMailer` to mimic email sending).\n",
    "- No Monetary Budget: Focus on free resources and time allocation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Approach\n",
    "\n",
    "### Types of Testing:\n",
    "- Unit Testing: Validate individual components of the **AIO - Course Planner** (e.g., authentication module, course search functionality, comment system).\n",
    "- Integration Testing: Test interactions between different modules (e.g., how adding courses affects timetable display, how study plan interacts with degree roadmap), from developers' point of view.\n",
    "- System Testing: Verify that the web-application works as a whole from the user's perspective, including complete workflows (planning timetable and study plan).\n",
    "- UI/UX Testing: Ensure the interface is clear, easy to use, and works correctly.\n",
    "- Performance Testing: Assess performance metrics, such as:\n",
    "    - Page load time should be under 2 seconds.\n",
    "    - API responses should be within 500 milliseconds.\n",
    "    - The app should support multiple concurrent users significant performance degrade.\n",
    "\n",
    "- Security Testing: Verify authentication mechanisms, user data protection, and access controls (such as comment deletion).\n",
    "- Compatibility Testing: Confirm that the web-application works correctly for different devices.\n",
    "- Regression Testing: Re-test system after adding new features or fixing bugs.\n",
    "\n",
    "### Methodologies:\n",
    "- Manual Testing: Primary approach for functionality verification, user experience, and exploratory testing since no testing framework is being used for the web application.\n",
    "- Browser Developer Tools: For frontend testing, performance analysis, and mobile responsiveness verification.\n",
    "- Database Verification: Using MongoDB Compass to verify data persistence and integrity.\n",
    "- Exploratory Testing: Discover edge cases manually.\n",
    "- Environment-Based Testing: Using Docker containers to ensure environment consistency across development, testing, and production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Timeline and Schedule\n",
    "\n",
    "### 1. Agile Model\n",
    "In our Agile model, testing is integrated throughout the development process with iterative cycles. Each sprint incorporates testing activities for newly developed features while maintaining regression testing for existing functionality.\n",
    "\n",
    "#### Dependencies:\n",
    "- MongoDB database connection for user data, course information, and study plans\n",
    "- Node.js/Express backend functionality\n",
    "- Frontend UI components and interactions\n",
    "- Docker environment for consistent testing and deployment\n",
    "\n",
    "#### Key Phases and Timeline:\n",
    "1. Sprint Planning and Preparation: Day 1 of each sprint\n",
    "    - Define acceptance criteria for features (authentication, course search, timetable, study plan)\n",
    "    - Create test cases for new functionality\n",
    "    - Set up test data and environments in Docker\n",
    "2. API Testing: Days 2-3 of each sprint\n",
    "    - Test authentication endpoints (/api/auth) for user signup, login, and token verification\n",
    "    - Validate course information endpoints (/api/courses) for retrieving course data\n",
    "    - Test study plan routes (/api/studyplan) for saving and retrieving user study plans\n",
    "    - Verify timetable routes (/api/timetable) for managing semester course selections\n",
    "3. Integration Testing: Days 6-7 of each sprint\n",
    "    - Test interaction between course selection and timetable visualization\n",
    "    - Verify study plan persistence in MongoDB and retrieval functionality\n",
    "    - Validate email verification token generation and processing\n",
    "    - Test user authentication flow across protected routes\n",
    "4. UI/UX Testing: Days 8-9 of each sprint\n",
    "    - Use Cypress (configured in cypress.config.js) to test frontend interactions\n",
    "    - Verify responsive design across different screen sizes\n",
    "    - Test course search, selection, and visualization in the timetable view\n",
    "    - Validate study plan creation and management interface\n",
    "5. End-of-Sprint Testing: Day 10 of each sprint\n",
    "    - Perform regression testing of core features using automated Cypress tests\n",
    "    - Conduct security testing for authentication and authorization\n",
    "    - Verify data persistence across all features (timetable, study plan)\n",
    "    - Document any remaining issues for the next sprint\n",
    "\n",
    "#### Agile Model Timeline (2-Week Sprint):\n",
    "\n",
    "| Day | Activity |\n",
    "|-----|-------------------|\n",
    "| Day 1 | Define test cases for new features (auth, courses, timetable, study plan) |\n",
    "| Day 2-3 | Test user authentication API endpoints and middleware |\n",
    "| Day 4-5 | Test course data retrieval and transformation from course-data directory |\n",
    "| Day 6-7 | Test study plan creation, storage, and retrieval in MongoDB |\n",
    "| Day 7 | Test timetable generation and conflict detection |\n",
    "| Day 8-9 | Run Cypress tests for frontend user flows and interfaces |\n",
    "| Day 9 | Perform regression testing on core functionality |\n",
    "| Day 10 | Document test results and plan for next sprint |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Risk Assessment and Mitigation\n",
    "\n",
    "### Potential Risks:\n",
    "1. Data Loss in Study Plans or Timetables:\n",
    "    - Risk: Users could lose carefully created course plans due to system errors, browser issues or network issues.\n",
    "    - Mitigation: Implement auto-save functionality that makes use of browser's `localStorage` module or cookie sessions.\n",
    "\n",
    "2. Course Data Inconsistency:\n",
    "    - Risk: Course information (schedules, prerequisites, descriptions) may be outdated.\n",
    "    - Mitigation: Implement auto data update functionality for backend/database system.\n",
    "\n",
    "3. Browser Compatibility Issues:\n",
    "    - Risk: For example, maybe drag-and-drop functionality for course planning might not work consistently across all browsers.\n",
    "    - Mitigation: Refactor whole codebase using frontend framework in future.\n",
    "\n",
    "4. Email Verification Challenges:\n",
    "    - Risk: Email verification tokens might not be delivered or processed correctly.\n",
    "    - Mitigation: Implement fallback verification options, and token resending functionality (This is implemented).\n",
    "\n",
    "5. Performance Under Load:\n",
    "    - Risk: System may slow down when handling many concurrent users or complex study plans.\n",
    "    - Mitigation: Optimize database queries, implement pagination where appropriate, and use profiling tools to analyze functions.\n",
    "\n",
    "6. Intrinsic Untestability of Course Conflict Detection:\n",
    "    - Risk: Complex schedule conflict detection algorithms may be difficult to test comprehensively.\n",
    "    - Mitigation: Create a suite of test cases covering various conflict scenarios, implement verbose logging in development mode, and conduct extensive manual testing of edge cases.\n",
    "\n",
    "7. Google Calendar Integration Issues:\n",
    "    - Risk: Exported timetable data might not format correctly for Google Calendar imports.\n",
    "    - Mitigation: Rigorously test the CSV export format against Google Calendar import specifications with various course combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Success Criteria\n",
    "The AIO - Course Planner testing will be considered successful when the following criteria are met:\n",
    "1. Functionality Objectives:\n",
    "    - All critical and high-priority bugs are resolved.\n",
    "    - All core features (timetable planning, study plan creation, course search, etc.) function as specified.\n",
    "    - Course information is displayed accurately and completely.\n",
    "    - Drag-and-drop course placement works reliably across supported browsers.\n",
    "    - Email verification system functions correctly.\n",
    "    - Google Calendar export produces valid, importable files.\n",
    "\n",
    "2. Performance Objectives:\n",
    "    - Page load times are within acceptable limits (under 2 seconds for main pages).\n",
    "    - API responses return within 500ms on average.\n",
    "    - Application remains responsive with multiple concurrent users.\n",
    "    - Course search returns results quickly (within 1 second).\n",
    "\n",
    "3. User Experience Objectives:\n",
    "    - Positive feedback from usability testing participants.\n",
    "    - New users can complete basic tasks without assistance.\n",
    "    - UI adapts appropriately to different screen sizes.\n",
    "    - Course data is visually clear and understandable.\n",
    "\n",
    "4. Security Objectives:\n",
    "    - User authentication works securely.\n",
    "    - User data is properly protected (email and passwords etc.).\n",
    "    - Email verification process is secure.\n",
    "\n",
    "5. Reliability Objectives:\n",
    "    - Study plans and timetables are saved and retrieved accurately.\n",
    "    - No data loss occurs during normal usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reporting Requirements\n",
    "\n",
    "### Documentation:\n",
    "- Test Case Documentation: Detailed description of each test case in the GitHub repository, including steps to reproduce, expected results, and actual results.\n",
    "- Bug Reports: Standardized bug reports with severity levels, steps to reproduce, screenshots or videos, and expected vs. actual behavior.\n",
    "- Feature Test Coverage Reports: Documentation showing which features have been tested and their pass/fail status. [darft: Highlight untested or partially tested areas for transparency. Markdown the status such as Not started, On-hold, In progress and Done]\n",
    "- Test Data: Sample course data, users, and study plans used for testing.\n",
    "- Environment Configuration: Provide clear instructions for setting up and running the test environment.\n",
    "- Testing Checklists: For manual tests of core user flows like course planning and timetable creation.\n",
    "\n",
    "### Communication:\n",
    "- Frequent Testing Updates/Meetings: Brief updates on testing progress, including the number of tests executed and bugs found.\n",
    "- Issue Tracking: Using Notion dashboard to track and manage bugs, with clear assignments and status updates.\n",
    "- User Acceptance Testing Feedback: Collection and reporting of feedback from UAT participants.\n",
    "- Final Test Summary Report: Comprehensive report before release, detailing test coverage, outstanding issues, and recommendations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
