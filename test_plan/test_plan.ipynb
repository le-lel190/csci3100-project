{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"CSCI3100: Software Engineering\"\n",
    "subtitle: \"Test Plan of AIO - Course Planner\"\n",
    "author: \"\"\n",
    "institute: \"Department of Computer Science And Engineering, The Chinese University of Hong Kong\"\n",
    "date: \"23 Apr 2025\"\n",
    "date-format: long\n",
    "format:\n",
    "  pdf:\n",
    "    toc: true\n",
    "    number-sections: true\n",
    "    colorlinks: true\n",
    "    include-in-header: \n",
    "      text: |\n",
    "        \\usepackage{fancyhdr}\n",
    "        \\pagestyle{fancy}\n",
    "execute:\n",
    "  echo: true\n",
    "  warning: true\n",
    "  output: true\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Revision History\n",
    "\n",
    "|Version| Revised By| Revision Date| Comments|\n",
    "|-------|-----------|--------------|---------|\n",
    "|1.0    | Anson     | 23 Apr 2025  | Initial draft| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test Plan for **AIO - Course Planner**\n",
    "\n",
    "This is a test plan for **AIO - Course Planner**. The most important sections include:\n",
    "\n",
    "1. Scope and Objectives --- This part states what will be (and what not will be) included in the testing.\n",
    "2. Test Cases and Scenarios --- This part explains each test case and scenario, by providing a detailed breakdown of the scope, the testing procedures and pass/fail criteria.\n",
    "3. Team Roles and Responsibilities --- This part shows the assignment of specific duties and accountabilities to each team member.\n",
    "4. Testing Approach & Timeline and Schedule --- This part outlines the approach and schedule of the tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scope and Objectives\n",
    "This part defines the scope of test cases. By having a clear definition, it avoids multiple test cases overlapping on certain areas.\n",
    "\n",
    "### Scope:\n",
    "The test plan focuses on ensuring the functionality, performance, and user experience of the **AIO - Course Planner**. The following areas will be tested:\n",
    "\n",
    "- Core mechanics (e.g., Timetable planning, Study Plan planner, Viewing degree roadmap, Course commenting).\n",
    "- User interface (UI) and user experience (UX).\n",
    "- Account system (Login, Logout, Registration).\n",
    "- Email token verification module.\n",
    "- Save/load functionality (for Timetable and Study Plan).\n",
    "- Third-party integration to Google Calendar.\n",
    "\n",
    "### Out of Scope:\n",
    "- Some hard-to-test aspects of the software (please see Risk assessment and mitigation).\n",
    "\n",
    "### Objectives:\n",
    "- Verify that the system meets functional and non-functional requirements, and provides a bug-free experience.\n",
    "- Ensure the system is intuitive, and free of critical bugs.\n",
    "- Confirm that the system performs well under various conditions (e.g., high user load, low-end devices). (TODO: FIX ????) [draft: The system can support around 100 concurrent users, which can be verified with Selenium testing.]\n",
    "- Validate that the system is ready for release and meets quality standards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Cases and Scenarios\n",
    "The section outlines comprehensive testing strategies for the **AIO - Course Planner** web application. It has test cases for both functional and non-functional test scenarios designed to ensure the application meets quality standards and user requirements. Each test case includes procedures, expected results, and criteria to guide manual testing efforts into the testing process.\n",
    "\n",
    "The test cases below use manual testing steps currently, which can serve as the foundation for future test automation efforts.\n",
    "\n",
    "### Test Cases for Functional Requirements:\n",
    "1. User Authentication:\n",
    "   - Steps: Register a new account, verify email, and login with credentials.\n",
    "   - Expected Result: User is registered successfully and can log in without email verification (it is required for course commenting only).\n",
    "   - Pass/Fail Criteria: Account is not created in database or users cannot log in.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC001\n",
    "       - Repository Link: [GitHub - auth.cy.js](https://github.com/le-lel190/csci3100-project/blob/dev_testing/cypress/e2e/auth.cy.js)\n",
    "\n",
    "2. Course Search and Selection:\n",
    "   - Steps: Search for courses by keyword, filter results, and view course details.\n",
    "   - Expected Result: Relevant courses are displayed and details are accessible.\n",
    "   - Pass/Fail Criteria: Course search returns accurate results and course information is complete.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC003\n",
    "       - Repository Link: [GitHub - course-search.cy.js](https://github.com/le-lel190/csci3100-project/blob/dev_testing/cypress/e2e/course-search.cy.js)\n",
    "       - Status: Automated with Cypress\n",
    "       - Coverage: Basic keyword search and semester filtering implemented\n",
    "\n",
    "3. Study Plan Creation:\n",
    "   - Steps: Add courses to study plan, modify sequence, and save changes.\n",
    "   - Expected Result: Study plan is updated and persisted correctly.\n",
    "   - Pass/Fail Criteria: Changes to study plan are saved and retrieved accurately on reload.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC004\n",
    "       - Repository Link: [GitHub - study-plan.cy.js](https://github.com/le-lel190/csci3100-project/blob/dev_testing/cypress/e2e/study-plan.cy.js)\n",
    "       - Status: Partially automated with Cypress\n",
    "       - Coverage: Adding courses and saving study plan implemented. Course reordering (drag-and-drop) currently skipped in automation.\n",
    "\n",
    "4. Timetable Management:\n",
    "   - Steps: Add courses to timetable, and detect conflicts.\n",
    "   - Expected Result: Courses appear in timetable at correct times with conflict warnings.\n",
    "   - Pass/Fail Criteria: Timetable visually displays all courses without UI errors.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC005\n",
    "       - Status: Manual testing only\n",
    "\n",
    "5. Comment System:\n",
    "   - Steps: Add comments to courses, edit/delete own comments, and view others' comments.\n",
    "   - Expected Result: Comments are saved and displayed correctly for all users.\n",
    "   - Pass/Fail Criteria: Comment operations complete without errors and update in real-time.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC006\n",
    "       - Status: Manual testing only\n",
    "\n",
    "6. Degree Roadmap Visualization:\n",
    "   - Steps: View degree roadmap, interact with course nodes, and check prerequisites.\n",
    "   - Expected Result: Roadmap displays correct course sequence with prerequisites highlighted.\n",
    "   - Pass/Fail Criteria: All course relationships are accurately represented in the visualization.\n",
    "   - Test cases:\n",
    "     - Test Case ID: TC007\n",
    "       - Status: Manual testing only\n",
    "\n",
    "\n",
    "### Test Cases for Non-Functional Requirements\n",
    "\n",
    "This part focus on test cases that assess the non-functional requirements focus on the quality attributes of the performance, scalability, usability, reliability, and security.\n",
    "\n",
    "#### 1. Performance Testing\n",
    "\n",
    "1. Page Load Time\n",
    "   - Objective: Verify that pages load within acceptable time limits.\n",
    "   - Steps:\n",
    "      1. Navigate to each main page (index, timetable, study planner, etc.)\n",
    "      2. Measure the time taken to fully load content.\n",
    "   - Expected Result: Pages load within 2 seconds on standard connections.\n",
    "   - Pass/Fail Criteria: Pass if load time meets expectations; fail otherwise.\n",
    "   - Status: Basic load testing implemented in 0-app-ready.cy.js\n",
    "\n",
    "2. API Response Time\n",
    "   - Objective: Ensure API endpoints respond quickly to client requests.\n",
    "   - Steps:\n",
    "      1. Make requests to key API endpoints (course search, authentication, etc.)\n",
    "      2. Measure response times under different load conditions.\n",
    "   - Expected Result: API responses return within 500ms on average.\n",
    "   - Pass/Fail Criteria: Pass if response times remain within acceptable thresholds.\n",
    "   - Status: Basic API health check in 0-app-ready.cy.js\n",
    "\n",
    "3. Simultaneous User Testing\n",
    "   - Objective: Test application behavior with many concurrent users.\n",
    "   - Steps:\n",
    "      1. Simulate multiple users accessing the application simultaneously.\n",
    "      2. Monitor server resource usage and response times.\n",
    "   - Expected Result: Application remains responsive with up to 100 simultaneous users.\n",
    "   - Pass/Fail Criteria: Pass if performance degradation is minimal under load.\n",
    "   - Status: Manual testing only, hard to test\n",
    "\n",
    "\n",
    "#### 2. Usability Testing\n",
    "\n",
    "1. Navigation Flow\n",
    "   - Objective: Verify that users can navigate the application intuitively.\n",
    "   - Steps:\n",
    "      1. Ask new users to complete common tasks without instructions.\n",
    "      2. Observe navigation patterns and points of confusion.\n",
    "   - Expected Result: Users can complete basic tasks without assistance.\n",
    "   - Pass/Fail Criteria: Pass if users accomplish tasks efficiently; fail if significant confusion occurs.\n",
    "\n",
    "2. Mobile Responsiveness\n",
    "   - Objective: Ensure the application is usable on mobile devices.\n",
    "   - Steps:\n",
    "      1. Access application on various mobile devices and screen sizes.\n",
    "      2. Test all major features on mobile browsers.\n",
    "   - Expected Result: UI adapts appropriately to screen size with all functionality accessible.\n",
    "   - Pass/Fail Criteria: Pass if mobile experience is consistent with desktop.\n",
    "\n",
    "#### 3. Reliability Testing\n",
    "\n",
    "1. Data Persistence\n",
    "   - Objective: Verify that user data is saved correctly and persistently.\n",
    "   - Steps:\n",
    "      1. Create study plans and timetables, then log out and back in.\n",
    "      2. Check that all user-created data is preserved.\n",
    "   - Expected Result: All user data is persistent across sessions.\n",
    "   - Pass/Fail Criteria: Pass if no data loss occurs; fail otherwise.\n",
    "\n",
    "2. Error Recovery\n",
    "   - Objective: Test application's ability to handle and recover from errors.\n",
    "   - Steps:\n",
    "      1. Simulate network disruptions during operations.\n",
    "      2. Intentionally submit invalid data to test error handling.\n",
    "   - Expected Result: Application provides meaningful error messages and recovers gracefully.\n",
    "   - Pass/Fail Criteria: Pass if app maintains usability after errors.\n",
    "\n",
    "\n",
    "#### 4. Security Testing\n",
    "\n",
    "1. Authentication Security\n",
    "   - Objective: Verify secure login and session management.\n",
    "   - Steps:\n",
    "      1. Attempt various authentication bypass techniques.\n",
    "      2. Test password strength requirements and account lockout functionality.\n",
    "   - Expected Result: Unauthorized access attempts are prevented.\n",
    "   - Pass/Fail Criteria: Pass if authentication barriers cannot be circumvented.\n",
    "\n",
    "2. Data Protection\n",
    "   - Objective: Ensure user data is protected.\n",
    "   - Steps:\n",
    "      1. Inspect network traffic for sensitive data transmission.\n",
    "      2. Test that API endpoints properly validate authentication.\n",
    "   - Expected Result: Sensitive data is encrypted and access-controlled.\n",
    "   - Pass/Fail Criteria: Pass if no unauthorized data access is possible.\n",
    "\n",
    "#### 5. Compatibility Testing\n",
    "\n",
    "1. Browser Compatibility\n",
    "   - Objective: Ensure functionality across major browsers.\n",
    "   - Steps:\n",
    "      1. Test application on Chrome, Firefox, Safari, and Edge.\n",
    "      2. Verify feature parity and visual consistency.\n",
    "   - Expected Result: Application works identically across all supported browsers.\n",
    "   - Pass/Fail Criteria: Pass if no browser-specific issues exist.\n",
    "\n",
    "2. Device Compatibility\n",
    "   - Objective: Verify application works on different devices and screen sizes.\n",
    "   - Steps:\n",
    "      1. Test on desktop, tablet, and mobile devices.\n",
    "      2. Check for layout or functionality issues.\n",
    "   - Expected Result: Application is fully functional across all device types.\n",
    "   - Pass/Fail Criteria: Pass if the experience is consistent across devices.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource Allocation\n",
    "This section outlines the personnel, tools, environments and other resources required to execute the test plan effectively.\n",
    "\n",
    "### 1. Team Roles and Responsibilities\n",
    "\n",
    "#### Key Team Members: (TODO: Fix this part cuz we just share the work...)\n",
    "1. Test Lead:\n",
    "    - Oversees the entire testing process.\n",
    "    - Creates test plans. \n",
    "    - Reports progress to stakeholders.\n",
    "    - Assigns tasks to team members and monitors progress.\n",
    "2. Manual Testers:\n",
    "    - Execute functional and non-functional test cases.\n",
    "    - Document results. \n",
    "    - Identity and report bugs.\n",
    "3. Frontend Developer:\n",
    "    - Assists with UI/UX testing.\n",
    "    - Validate the user interface and user experience. \n",
    "    - Resolves front-end related issues.\n",
    "4. Backend Developer:\n",
    "    - Assists with API testing. \n",
    "    - Resolves server-side issues.\n",
    "5. Database Administrator:\n",
    "    - Manages database configurations.\n",
    "\n",
    "#### Resource Allocation Table: (TODO: Fix this)\n",
    "\n",
    "| Role | Name | Responsibilities | Contribution (%) |\n",
    "|--------|------------------|----------------------|------------------|\n",
    "| Test Lead | Cheung Wai Lok |Oversee testing, coordination, progress tracking | 12.5% |\n",
    "| Manual Testers | All members | Performance testing, log bugs | 12.5% |\n",
    "| Frontend Developer | Lam Leung Yiu | Validate UI/UX and accessibility | 12.5% |\n",
    "| Backend Developer | Cheung Ching Yu | Validate features and resolve identified issues | 12.5% |\n",
    "| Database Administrator | Liu Yun Hei |Database Design and Maintenance | 12.5% |\n",
    "\n",
    "\n",
    "### 2. Tools and Software\n",
    "\n",
    "#### Testing Tools:\n",
    "- Browser Developer Tools: Chrome DevTools for frontend testing.\n",
    "- MongoDB Compass: For database inspection and verification.\n",
    "- Manual Testing: Since no framework is used for webapp building.\n",
    "- Git: For version controlling test scripts/datasets/files and documents.\n",
    "\n",
    "### 3. Testing Environments\n",
    "\n",
    "#### Environment Types:\n",
    "- Virtual Machine (Docker): Both developing, testing and production environments are containerized by Docker to avoid endpoint discrepency.\n",
    "- Developing environment: Consists of test files and source codes, used by developers for developing, testing and debugging.\n",
    "- Production Environment: Used for final validation during User Acceptance Testing. Mimics the live environment to ensure the system is ready for release.\n",
    "\n",
    "#### Environment Setup:\n",
    "- Hardware Requirements:\n",
    "    - Devices/Browsers/DevTool mimicing to test behaviour of the web-application in desktop and laptop environments.\n",
    "\n",
    "- Software Requirements:\n",
    "    - Docker (Compose) Containers: For consistent testing environments.\n",
    "    - Node.js: The backend of the web-application.\n",
    "    - MongoDB Database: Stores user data and comments.\n",
    "\n",
    "#### Environment Allocation Table: (TODO: Fix this)\n",
    "\n",
    "| Environment | Purpose | Configuration | Responsibility |\n",
    "|-------------|---------|--------------|----------------|\n",
    "| Local | Individual feature testing | Local installations on each member's computer | Individual Team Members |\n",
    "| Shared Test | Coordinated testing | Docker compose setup accessible to all members | Group Leader + Team Member with Docker knowledge |\n",
    "| Production-like | Final validation | Deployment to free tier hosting (if available) | Group Leader |\n",
    "\n",
    "\n",
    "### 4. Time Allocation\n",
    "\n",
    "#### Effort Estimation: (TODO: Fix this later)\n",
    "\n",
    "- Effort Estimation:\n",
    "    1. Test Planning: 3-4 days teseting \n",
    "    2. Test Case Development: 1 week\n",
    "    3. Functional Testing: 1-2 weeks\n",
    "    4. Non-functional Testing: 1 week\n",
    "    5. Bug Fixing and Verification: Ongoing throughout project\n",
    "\n",
    "#### Time Allocation Table: (TODO: Fix this later)\n",
    "\n",
    "| Testing Phase | Duration | Team Members Involved | Deliverables |\n",
    "|---------------|----------|----------------------|--------------|\n",
    "| Test Planning | 3-4 days | All team members, led by Group Leader | Test Plan Document |\n",
    "| Test Case Development | 1 week | All team members | Test Cases, Test Data |\n",
    "| Authentication Testing | 2-3 days | Team Member 1, supported by others | Test Results, Bug Reports |\n",
    "| Course Management Testing | 2-3 days | Team Member 2, supported by others | Test Results, Bug Reports |\n",
    "| Study Plan & Timetable Testing | 3-4 days | Team Member 3, supported by others | Test Results, Bug Reports |\n",
    "| UI/UX Testing | 2-3 days | Group Leader, supported by others | Test Results, Bug Reports |\n",
    "| Performance & Security Testing | 2-3 days | Shared responsibility | Performance Report |\n",
    "| Compatibility Testing | 1-2 days | Team Member 2, supported by others | Compatibility Report |\n",
    "| Bug Fixing & Verification | Ongoing | All team members | Updated Application |\n",
    "\n",
    "### 5. Budget Allocation\n",
    "\n",
    "#### Key Budget Considerations:\n",
    "- Time Investment: Primary resource is team members' time.\n",
    "- Free Tools: Utilizing free versions instead of premium versions of software/services.\n",
    "- Personal Computing Resources: Using team members' own computers.\n",
    "- Cloud Resources: Minimal use of free tier cloud services if needed (e.g. using `EtherealMail` + `NodeMailer` to mimic email sending).\n",
    "- No Monetary Budget: Focus on free resources and time allocation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Approach\n",
    "\n",
    "### Types of Testing:\n",
    "- Unit Testing: Validate individual components of the **AIO - Course Planner** (e.g., authentication module, course search functionality, comment system).\n",
    "- Integration Testing: Test interactions between different modules (e.g., how adding courses affects timetable display, how study plan interacts with degree roadmap), from developers' point of view.\n",
    "- System Testing: Verify the application as a whole from the user's perspective, including complete workflows like creating a study plan or generating a timetable.\n",
    "- UI/UX Testing: Ensure the interface is intuitive, responsive, and functions correctly across all pages.\n",
    "- Performance Testing: Evaluate load times, API response times, and behavior under concurrent user scenarios. \n",
    "    - Page load time should be under 2 seconds.\n",
    "    - API responses should take less than 500 milliseconds.\n",
    "    - The app should support 100 concurrent users without crashing or slowing down significantly.\n",
    "    - (TODO:[draft: Define performance targets (e.g., page load <2s, API response <500ms, handle 100 users). Document findings and bottlenecks for improvement.]\n",
    "\n",
    "- Security Testing: Verify authentication mechanisms, user data protection, and access controls.\n",
    "- Compatibility Testing: Confirm functionality across different browsers and devices.\n",
    "- Regression Testing: Ensure that new features or bug fixes don't break existing functionality.\n",
    "\n",
    "### Methodologies:\n",
    "- Manual Testing: Primary approach for functionality verification, user experience, and exploratory testing since no testing framework is being used for the web application.\n",
    "- Browser Developer Tools: For frontend testing, performance analysis, and mobile responsiveness verification.\n",
    "- Database Verification: Using MongoDB Compass to verify data persistence and integrity.\n",
    "- Exploratory Testing: To discover edge cases and unexpected behaviors, particularly in complex workflows like timetable conflict detection. (TODO: ??) [draft: Document steps, findings, and edge cases discovered during each session.]\n",
    "- Environment-Based Testing: Using Docker containers to ensure consistency across development, testing, and production environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Timeline and Schedule (TODO: This part NEEDS SEVERE FIX)\n",
    "\n",
    "### 1. Agile Model\n",
    "In our Agile model, testing is integrated throughout the development process with iterative cycles. Each sprint incorporates testing activities for newly developed features while maintaining regression testing for existing functionality.\n",
    "\n",
    "#### Dependencies:\n",
    "- Completion of feature development within each sprint.\n",
    "- Availability of testing environments (Docker containers + Compose environment settings).\n",
    "- Access to MongoDB for data verification.\n",
    "- Timely resolution of reported bugs.\n",
    "\n",
    "#### Key Phases and Timeline:\n",
    "1. Sprint Planning and Preparation: Day 1 of each sprint\n",
    "- Define acceptance criteria for course planner features (timetable, study plan, etc.)\n",
    "- Create test cases for new functionality\n",
    "- Set up test data and environments in Docker\n",
    "2. Testing During the Sprint: Ongoing (Daily)\n",
    "- Execute test cases for completed features\n",
    "- Perform exploratory testing on UI/UX elements\n",
    "- Log bugs in course search, timetable conflicts, study plan persistence\n",
    "- Conduct regression testing for authentication and previously implemented features\n",
    "3. Mid-Sprint Testing Milestones: Midpoint of each sprint\n",
    "- Perform integration testing between modules (e.g., how course selection affects timetable)\n",
    "- Update test cases based on interim feedback and UI/UX adjustments\n",
    "- Verify data persistence in MongoDB\n",
    "4. End-of-Sprint Testing Activities: Last 1–2 days of each sprint\n",
    "- Conduct user acceptance testing for completed features\n",
    "- Perform cross-browser compatibility testing\n",
    "- Execute performance tests for API response times and page loads\n",
    "5. Post-Sprint Activities: After each sprint\n",
    "- Update the test suite with new test cases\n",
    "- Document known issues and workarounds\n",
    "- Plan testing activities for the next sprint\n",
    "- [draft: update the progress of the last sprint]\n",
    "\n",
    "#### Agile Model Timeline for a 2-Week Sprint:\n",
    "\n",
    "| Day | Activity |\n",
    "|---------|------------------------------------------------------------------------------|\n",
    "| Day 1 | Sprint planning, define acceptance criteria for new course planner features |\n",
    "| Day 2 | Prepare test data (course information, schedules), configure test environment |\n",
    "| Day 3–4 | Test user authentication, course search, and selection features |\n",
    "| Day 5 | Integration testing between course selection and timetable visualization |\n",
    "| Day 6–7 | Test study plan creation and persistence, verify email token functionality |\n",
    "| Day 8 | Test Google Calendar integration, verify data export functionality |\n",
    "| Day 9 | Perform UI/UX testing on multiple browsers and devices |\n",
    "| Day 10 | Sprint review/demo, verify all completed features work in the integrated environment |\n",
    "| Day 11 | Regression testing for core features (authentication, timetable, study plan) |\n",
    "| Day 12 | Document test results, update test cases, collaborate on next sprint planning |\n",
    "\n",
    "[darft: ]\n",
    "| Day | Activity |\n",
    "|---------|------------------------------------------------------------------------------|\n",
    "| Day 1 | Set sprint goals and clarify acceptance criteria for new course planner features |\n",
    "| Day 2 | Prepare and validate test data, and set up the test environment |\n",
    "| Day 3 | Complete testing of user authentication functionality |\n",
    "| Day 4 | Finish testing course search and selection features |\n",
    "| Day 5 | Integrate and test course selection with timetable visualization |\n",
    "| Day 6 | Test study plan creation feature |\n",
    "| Day 7 | Ensure study plan data is saved and persists correctly |\n",
    "| Day 8 | Test email token generation and verification |\n",
    "| Day 9 | Complete testing of Google Calendar integration |\n",
    "| Day 10 | Verify data export functionality |\n",
    "| Day 11 | Conduct UI/UX testing across different browsers and devices |\n",
    "| Day 12 | Review and demo all completed features in the integrated environment |\n",
    "| Day 13 | Perform regression testing on core features and document results |\n",
    "| Day 14 | Gather feedback, hold sprint retrospective, and plan improvements for the next sprint |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Risk Assessment and Mitigation\n",
    "\n",
    "### Potential Risks:\n",
    "1. Data Loss in Study Plans or Timetables:\n",
    "    - Risk: Users could lose carefully created course plans due to system errors, browser issues or network issues.\n",
    "    - Mitigation: Implement auto-save functionality that makes use of browser's `localStorage` module or cookie sessions.\n",
    "\n",
    "2. Course Data Inconsistency:\n",
    "    - Risk: Course information (schedules, prerequisites, descriptions) may be outdated.\n",
    "    - Mitigation: Implement auto data update functionality for backend/database system.\n",
    "\n",
    "3. Browser Compatibility Issues:\n",
    "    - Risk: For example, maybe drag-and-drop functionality for course planning might not work consistently across all browsers.\n",
    "    - Mitigation: Refactor whole codebase using frontend framework in future.\n",
    "\n",
    "4. Email Verification Challenges:\n",
    "    - Risk: Email verification tokens might not be delivered or processed correctly.\n",
    "    - Mitigation: Implement fallback verification options, and token resending functionality (This is implemented).\n",
    "\n",
    "5. Performance Under Load:\n",
    "    - Risk: System may slow down when handling many concurrent users or complex study plans.\n",
    "    - Mitigation: Optimize database queries, implement pagination where appropriate, and use profiling tools to analyze functions.\n",
    "\n",
    "6. Intrinsic Untestability of Course Conflict Detection:\n",
    "    - Risk: Complex schedule conflict detection algorithms may be difficult to test comprehensively.\n",
    "    - Mitigation: Create a suite of test cases covering various conflict scenarios, implement verbose logging in development mode, and conduct extensive manual testing of edge cases.\n",
    "\n",
    "7. Google Calendar Integration Issues:\n",
    "    - Risk: Exported timetable data might not format correctly for Google Calendar imports.\n",
    "    - Mitigation: Rigorously test the CSV export format against Google Calendar import specifications with various course combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Success Criteria\n",
    "The AIO - Course Planner testing will be considered successful when the following criteria are met:\n",
    "1. Functionality Objectives:\n",
    "    - All critical and high-priority bugs are resolved.\n",
    "    - All core features (timetable planning, study plan creation, course search, etc.) function as specified.\n",
    "    - Course information is displayed accurately and completely.\n",
    "    - Drag-and-drop course placement works reliably across supported browsers.\n",
    "    - Email verification system functions correctly.\n",
    "    - Google Calendar export produces valid, importable files.\n",
    "\n",
    "2. Performance Objectives:\n",
    "    - Page load times are within acceptable limits (under 2 seconds for main pages).\n",
    "    - API responses return within 500ms on average.\n",
    "    - Application remains responsive with multiple concurrent users.\n",
    "    - Course search returns results quickly (within 1 second).\n",
    "\n",
    "3. User Experience Objectives:\n",
    "    - Positive feedback from usability testing participants.\n",
    "    - New users can complete basic tasks without assistance.\n",
    "    - UI adapts appropriately to different screen sizes.\n",
    "    - Course data is visually clear and understandable.\n",
    "\n",
    "4. Security Objectives:\n",
    "    - User authentication works securely.\n",
    "    - User data is properly protected (email and passwords etc.).\n",
    "    - Email verification process is secure.\n",
    "\n",
    "5. Reliability Objectives:\n",
    "    - Study plans and timetables are saved and retrieved accurately.\n",
    "    - No data loss occurs during normal usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reporting Requirements\n",
    "\n",
    "### Documentation: (TODO: FIX THIS)\n",
    "- Test Case Documentation: Detailed description of each test case in the GitHub repository, [draft: Notion workplace and discord], including steps to reproduce, expected results, and actual results.\n",
    "- Bug Reports: Standardized bug reports with severity levels, steps to reproduce, screenshots or videos, and expected vs. actual behavior.\n",
    "- Feature Test Coverage Reports: Documentation showing which features have been tested and their pass/fail status. [darft: Highlight untested or partially tested areas for transparency. Markdown the status such as Not started, On-hold, In progress and Done]\n",
    "- Test Data: Sample course data, users, and study plans used for testing.\n",
    "- Environment Configuration: Documentation of the Docker setup used for testing. [draft: Provide clear instructions for setting up and running the test environment on README.md.]\n",
    "- Testing Checklists: For manual tests of core user flows like course planning and timetable creation.\n",
    "\n",
    "### Communication:\n",
    "- Frequent Testing Updates/Meetings: Brief updates on testing progress, including the number of tests executed and bugs found.\n",
    "- Issue Tracking: Using Notion dashboard to track and manage bugs, with clear assignments and status updates.\n",
    "- User Acceptance Testing Feedback: Collection and reporting of feedback from UAT participants.\n",
    "- Final Test Summary Report: Comprehensive report before release, detailing test coverage, outstanding issues, and recommendations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
